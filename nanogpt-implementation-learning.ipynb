{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is meant to be run exclusively on CPU, not GPU using cuda. The changes are not drastic though to switch between the two, they require some conditional logic and pushing the parameters to device, but nothing huge. Loss is only calculated once, not averaged, this is a necessary optimization though\n",
      "This model is decoder only, so lacks encoder part of Attention is all you need paper, and lacks cross-attention\n",
      "This model is just a document completer, not a question answerer: need to fine-tune beyond the pretraining here to accomplish this\n"
     ]
    }
   ],
   "source": [
    "print(\"This model is meant to be run exclusively on CPU, not GPU using cuda. The changes are not drastic though to switch between the two, they require some conditional logic and pushing the parameters to device, but nothing huge. Loss is only calculated once, not averaged, this is a necessary optimization though\")\n",
    "print(\"This model is decoder only, so lacks encoder part of Attention is all you need paper, and lacks cross-attention\")\n",
    "print(\"This model is just a document completer, not a question answerer: need to fine-tune beyond the pretraining here to accomplish this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "enc = {ch:i for i, ch in enumerate(chars)} #iterators via enumerate\n",
    "dec = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [enc[c] for c in s] #serves as our encoder to convert characters in a sequence to list of numbers\n",
    "decode = lambda l: ''.join([dec[i] for i in l]) #conv list of integers into a string (decoder)\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n",
    "#conversion both ways complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "above is a character level tokenizer scheme, small vocab of only 65 letters in the shakespeare dataset: but can use tiktoken by openAI or sentencepiece by google to create a more robust encoder/decoder scheme\n"
     ]
    }
   ],
   "source": [
    "print(\"above is a character level tokenizer scheme, small vocab of only 65 letters in the shakespeare dataset: but can use tiktoken by openAI or sentencepiece by google to create a more robust encoder/decoder scheme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "#print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a couple global vars for rest of book\n",
    "n_embd = 128\n",
    "dropout = 0.2\n",
    "#number of embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(0.9*len(data))\n",
    "train = data[:cutoff]\n",
    "val = data[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_window = 64\n",
    "train[:context_window+1] #to enable offsets for prediction/target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trains context length times with varying context lengths from 1 to context length for training efficiency, but inference will be less efficient off many softmax PD predictions going to 'waste' for inference use case\n"
     ]
    }
   ],
   "source": [
    "x = train[:context_window]\n",
    "y = train[1:context_window+1]\n",
    "for i in range(context_window):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    #print(f\"input {context} target {target}\")\n",
    "print(\"trains context length times with varying context lengths from 1 to context length for training efficiency, but inference will be less efficient off many softmax PD predictions going to 'waste' for inference use case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______\n"
     ]
    }
   ],
   "source": [
    "#batching for parallelization\n",
    "#torch.manual_seed(1337)\n",
    "batchsize = 32 #num parallel sequences being processed\n",
    "cwbatch = 64 #context window max len\n",
    "def get_batch(splitname):\n",
    "    #create batches of data\n",
    "    data = train if splitname == \"train\" else val\n",
    "    idx = torch.randint(len(data) - cwbatch, (batchsize,))\n",
    "    x =  torch.stack([data[i:i+cwbatch] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+1+cwbatch] for i in idx])\n",
    "    return x, y\n",
    "xbatch, ybatch = get_batch('train')\n",
    "#print(\"inputs:\")\n",
    "#print(xbatch.shape)\n",
    "#print(xbatch)\n",
    "#print(\"targets:\")\n",
    "#print(ybatch.shape)\n",
    "#print(ybatch)\n",
    "\n",
    "print(\"______\")\n",
    "for b in range(batchsize):\n",
    "    for t in range(cwbatch):\n",
    "        context = xbatch[b:t+1]\n",
    "        target = ybatch[b, t]\n",
    "        #print(f\"input: {context.tolist()} target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (copy pasted, normalizes rows instead of columns as in BatchNorm)\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "#torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 65])\n",
      "tensor(4.2550, grad_fn=<NllLossBackward0>)\n",
      "expected loss for negative log likelihood of -ln(1/65) ~ 4.17\n",
      "\n",
      "3WfPa\n",
      "Z..wO aVc\n",
      "i$g;tAhCA;t?!:TtGgyt-zt$3iZUDWoAUrMW!nuPW.oMxzujsqHfxqk'hU,EjoMTzHOgW.ikju;3SxlTXLCf\n"
     ]
    }
   ],
   "source": [
    "#create bigram lm\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "#torch.manual_seed(1337)\n",
    "class Head(nn.Module):\n",
    "    #one head of self attention\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_window, context_window)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    '''B, T, C = 4, 8, 32 #batch, time, channels - 4 by 8 arrangement of tokens and each token is currently 32 dimenisional\n",
    "    #x = torch.randn(B, T, C)\n",
    "\n",
    "    #single attention head\n",
    "    head_size = 16\n",
    "    query = nn.Linear(C, head_size, bias = False)\n",
    "    key = nn.Linear(C, head_size, bias = False)\n",
    "    value = nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "    #tril = torch.tril(torch.ones(T, T))\n",
    "    #wei = torch.zeros((T, T))\n",
    "\n",
    "    the following line is very important. in autoregressive settings like LLMS, you mask future tokens from being used in the\n",
    "    context of a previous token. Hence why the masked fill line is present. In cases like sentiment\n",
    "    analysis, you may not want information to be masked, and rather have every token talk to every other\n",
    "    token in the sequence, so you would just delete the directly following masked fill line. \n",
    "    The autoregressive method with masked fill is known as a decoder attention block, sentiment like \n",
    "    full context without traingular masking is known as an encoder attention block\n",
    "    wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "\n",
    "    wei - F.softmax(wei, dim = -1)\n",
    "    v = value(x)\n",
    "    out = wei @ v\n",
    "    #out = wei @ x\n",
    "    out.shape'''\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) #(B, T, 16) where 16 is head size aka B, T, C\n",
    "        q = self.query(x) #(B, T, 16)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 #only transpose the last two since we need the B vector, normalize it\n",
    "        #--> (B, T, 16) @ (B, 16, T) --> (B, T, T) (makes the weights data dependent and not just uniform)\n",
    "        '''the following line is very important. in autoregressive settings like LLMS, you mask future tokens from being used in the\n",
    "        context of a previous token. Hence why the masked fill line is present. In cases like sentiment\n",
    "        analysis, you may not want information to be masked, and rather have every token talk to every other\n",
    "        token in the sequence, so you would just delete the directly following masked fill line. \n",
    "        The autoregressive method with masked fill is known as a decoder attention block, sentiment like \n",
    "        full context without traingular masking is known as an encoder attention block'''\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1) #(B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out  \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    #multiple self-attention heads running in parallel\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()        \n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd) #linear transformation of the outcome of the concatennation within MHA\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1) #concatenate results of the parallelly running attention heads over the channel dimension\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), #4 multiplied pursuant to the original Attnetion is all you need paper 512 --> 2048 --> 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), #residual layer\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class Block(nn.Module):\n",
    "    #communication then computation\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        #n_head = number of heads we want\n",
    "        super().__init__()\n",
    "        head_size = n_embd //n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #create a vocab by vocab sized embedding table - each token reads off the logits for the following token based on this table\n",
    "        #box 1: token encoding is complete, now box2: positional encoddings\n",
    "        self.position_embedding_table = nn.Embedding(context_window, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            nn.LayerNorm(n_embd)\n",
    "        )\n",
    "        #self.selfattentionhead = MultiHeadAttention(4, n_embd//4) #4 heads of 8-dimensional self-attention\n",
    "        #self.ffwd = FeedForward(n_embd)\n",
    "        self.languagemodellinghead = nn.Linear(n_embd, vocab_size)\n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx and targets are both (B, T) tensor of integers\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        token_embeddings = self.token_embedding_table(idx) #(Batch, Time, Channel) AKA B,T,C where B = batchsize (4), T = cwbatch = context window = 8, channel = vocab_size (65)\n",
    "        positional_embeddings = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_embeddings + positional_embeddings  #(B, T, C)\n",
    "        #x = self.selfattentionhead(x) #apply one head of self-attention outputting answer in format (B,T,C)\n",
    "        #x = self.ffwd(x) #(B, T, C)\n",
    "        x = self.blocks(x) #(B, T, C)\n",
    "        logits = self.languagemodellinghead(x) #(B, T, vocabsize) - this step applies the decoder language model we have to create logits from the output of the previous line\n",
    "        if targets is None: #optional to syncronize with the generate call through self (self in genertate calls forward)\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            #merge B and T dimensions into one dimensional vector in order to match pytorch requirement for C to be the second parameter for the cross entropy fn\n",
    "            loss = F.cross_entropy(logits, targets) #assess accuracy of logits (predictions) relative to the targets - correct dimension of logits should be a high number, rest very low number (essentaily unnormalized prob dists.)\n",
    "        return logits, loss\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is (B,T) array of indicies here - represents the current context of some characters in some batch\n",
    "        #generates one by one increase in tokens and appends this to the running sequence - go from B x T to B x T + 1, B x T + 2\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -batchsize:] #crop idx to the last batch_size number of tokens to ensure positional encodings are within rangae\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] #only last time step to become (B, C)\n",
    "            probs = F.softmax(logits, dim = -1) #softmax --> prb dist. (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) #(B, 1 - single prediction for what comes next in each of the batch dimensions)\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) #(B, T+1) #extra for bigram model since that relies on prev token prediction and this concatenation \n",
    "            #passes in the entire accumulated string into the model each time to predict the next token, but this generate function is meant to be generalizable \n",
    "            #to different models that want/require more context than just the preceding character to predict the next character (can swap character/token here depending on tokenization scheme used)\n",
    "\n",
    "        return idx\n",
    "m = BigramLM(vocab_size)\n",
    "logits, loss = m(xbatch, ybatch)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "print(\"expected loss for negative log likelihood of -ln(1/65) ~ 4.17\")\n",
    "#generates logits (scores) for every one of the 4 by 8 positions\n",
    "\n",
    "# idx portion: batch and time are 1 --> tensor is 1 x 1 and holdds a 0. 0 kicks off the generation (represents new line char)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 100)[0].tolist())) #indexing into 0th row indexes into the single batch dimension that exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5416239500045776\n"
     ]
    }
   ],
   "source": [
    "#batch_size = 32 update to be bigger\n",
    "for steps in range(5000):\n",
    "    xbatch, ybatch = get_batch('train')\n",
    "    logits, loss = m(xbatch, ybatch)\n",
    "    optimizer.zero_grad(set_to_none = True) #zero out gradients from the previous step \n",
    "    loss.backward() #get those gradients for all the parameters\n",
    "    optimizer.step() #using the gradients to update the parameters\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our marrs\n",
      "For good comt child pritse the worst:\n",
      "You would balievaltes.\n",
      "Is this envictor finst like a puned offence:\n",
      "He the guits welge wells herefore.\n",
      "Firstamed it ado much tears fortended,\n",
      "Ichar deukin fouguits dostingfit all cousing.\n",
      "Theself.\n",
      "\n",
      "ANGELIZABET:\n",
      "Thou stand in the shonour, leavene you he have child aduction\n",
      "Which intisforess fully fraith hate charmful LadyY\n",
      "\n",
      "Now:\n",
      "Extand spearous.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Why say, there mee protriansous-upoes,\n",
      "I kine.\n",
      "\n",
      "GREEN:\n",
      "Sir, Herein you hencle, is defio,\n",
      "Sun peach; and and mond our each'd\n",
      "Of did vithal, as I tell granion;\n",
      "The prince; and they good gifts,\n",
      "Oxforted with wordst burn be gody,\n",
      "To comfroance thee be Richal spuersate,\n",
      "That good like by forken this should,\n",
      "My layer's sbeen stand dons, but itranchands dighance.\n",
      "\n",
      "NORTENSIO:\n",
      "For\n",
      "That on inf oh hears and her maymany you?\n",
      "\n",
      "WARWICK:\n",
      "My lord, thou wantstage\n",
      "Asse king, this all this that beenf;\n",
      "It with Mercius, whehink, he againly day\n",
      "So hhis spoking, my freme, these shame, on thy patar?\n",
      "Good world?\n",
      "\n",
      "This Edward Gentlemengs, unhald him.\n",
      "\n",
      "First:\n",
      "I'll soubour.\n",
      "\n",
      "MENENIUS:\n",
      "The teat, agay, a him, to throather of broist: if helir's thou nalsed offerewell, speak.\n",
      "\n",
      "Provost Murden, if is a thing it compe,\n",
      "A tend and compaint lover you, and comes ond's labunds.\n",
      "\n",
      "QUEEN EDWARD IV:\n",
      "Freedemend is a wasant of it, persome by word? oncersur: home\n",
      "To your home.\n",
      "\n",
      "QUEEN:\n",
      "The happ is a fortuness; and some which I talk.\n",
      "\n",
      "LADY ANGELO:\n",
      "Ceet's I'll fortward youghful wonds.\n",
      "\n",
      "SIRITA:\n",
      "Pile namely good my gook ster more\n",
      "That the sucwixon foul to out guess, try,\n",
      "Stand that not that in conve any wonterity the\n",
      "Beecunnerate towilm'd at be you: in they artquoest urman,\n",
      "Even where men'd not regarent the chore?\n",
      "\n",
      "LADY BIH:\n",
      "sould this neitherous shand, by some on will ocallus,\n",
      "From I cange the lives broathe!\n",
      "Nor may good let's hands, bread in thelese our read miver;\n",
      "Who wren Bo\n",
      "thingh or my's henced, I mosting doth faint\n",
      "An give sast love which a might and pity\n",
      "A kisse roain oblachorts a worty,\n",
      "And we his not \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 2000)[0].tolist())) #indexing into 0th row indexes into the single batch dimension that exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#masked self attention below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2934, -1.2048],\n",
       "        [ 1.3275, -0.5738],\n",
       "        [-0.8724,  0.9453],\n",
       "        [ 1.0937, -0.9218],\n",
       "        [-2.5540, -0.4727],\n",
       "        [ 1.6102, -1.1374],\n",
       "        [-1.8820, -0.7011],\n",
       "        [ 0.9065,  0.8109]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #v1\n",
    "B, T, C = 4, 8 , 2\n",
    "x = torch.randn(B, T, C)\n",
    "xbagofwords = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # shape (t,C)\n",
    "        xbagofwords[b, t] = torch.mean(xprev, 0)\n",
    "#simple averaging C over all prev words - not effective since very lossy, and inefficient without matrix math\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2934, -1.2048],\n",
       "        [ 1.3105, -0.8893],\n",
       "        [ 0.5828, -0.2778],\n",
       "        [ 0.7105, -0.4388],\n",
       "        [ 0.0576, -0.4456],\n",
       "        [ 0.3164, -0.5609],\n",
       "        [ 0.0023, -0.5809],\n",
       "        [ 0.1154, -0.4069]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbagofwords[0]\n",
    "#average of all previous moving down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v2\n",
    "weights = torch.tril(torch.ones(T, T))\n",
    "#create triangular matrix for averaging in conjunction with next step\n",
    "weights = weights/weights.sum(1, keepdim = True)  #rows sum to 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbagofwords2 = weights @ x #(T,T) @ (B, T, C) --> pytorch will add a batch dimension (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "torch.allclose(xbagofwords, xbagofwords2) #now will be the same after the prev step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #v3 using softmax\n",
    "tril = torch.tril(torch.ones(T, T)) #lower triangular matrix of 1s\n",
    "weights = torch.zeros((T, T)) #all 0s initially\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) #assign tril 0s to be - infinity\n",
    "weights = F.softmax(weights, dim = -1) #take softmax along every single row - normalizes and spits back the same matrix for xbagofwords and xbagofwords3\n",
    "xbagofwords3 = weights @ x\n",
    "torch.allclose(xbagofwords, xbagofwords3)\n",
    "#this is the preferred version for self-attention since the pd generated by the softmax demonstartes impact of certain tokens communicating with others to influence their context\n",
    "#the weights are data-dependent, goes to the Q, K, V, context-rich vector mechanisms in the multihead triangularly masked self attention mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def forward(self,x):\\n    B,T,C = x.shape\\n    k = self.key(x) #(B, T, 16) where 16 is head size aka B, T, C\\n    q = self.query(x) #(B, T, 16)\\n    wei = q @ k.transpose(-2, -1) * C**-0.5 #only transpose the last two since we need the B vector, normalize it\\n#--> (B, T, 16) @ (B, 16, T) --> (B, T, T) (makes the weights data dependent and not just uniform)\\n    the following line is very important. in autoregressive settings like LLMS, you mask future tokens from being used in the\\n    context of a previous token. Hence why the masked fill line is present. In cases like sentiment\\n    analysis, you may not want information to be masked, and rather have every token talk to every other\\n    token in the sequence, so you would just delete the directly following masked fill line. \\n    The autoregressive method with masked fill is known as a decoder attention block, sentiment like \\n    full context without traingular masking is known as an encoder attention block\\n    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\\n    wei = F.softmax(wei, dim = -1) #(B, T, T)\\n    v = self.value(x)\\n    out = wei @ v\\n    return out  \""
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v4: self attention\n",
    "'''torch.manual_seed(1337)\n",
    "class Head(nn.Module):\n",
    "    #one head of self attention\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(batch_size, batch_size)))'''\n",
    "'''B, T, C = 4, 8, 32 #batch, time, channels - 4 by 8 arrangement of tokens and each token is currently 32 dimenisional\n",
    "#x = torch.randn(B, T, C)\n",
    "\n",
    "#single attention head\n",
    "head_size = 16\n",
    "query = nn.Linear(C, head_size, bias = False)\n",
    "key = nn.Linear(C, head_size, bias = False)\n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "#tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T, T))\n",
    "\n",
    "the following line is very important. in autoregressive settings like LLMS, you mask future tokens from being used in the\n",
    "context of a previous token. Hence why the masked fill line is present. In cases like sentiment\n",
    "analysis, you may not want information to be masked, and rather have every token talk to every other\n",
    "token in the sequence, so you would just delete the directly following masked fill line. \n",
    "The autoregressive method with masked fill is known as a decoder attention block, sentiment like \n",
    "full context without traingular masking is known as an encoder attention block\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "\n",
    "wei - F.softmax(wei, dim = -1)\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "out.shape'''\n",
    "'''def forward(self,x):\n",
    "    B,T,C = x.shape\n",
    "    k = self.key(x) #(B, T, 16) where 16 is head size aka B, T, C\n",
    "    q = self.query(x) #(B, T, 16)\n",
    "    wei = q @ k.transpose(-2, -1) * C**-0.5 #only transpose the last two since we need the B vector, normalize it\n",
    "#--> (B, T, 16) @ (B, 16, T) --> (B, T, T) (makes the weights data dependent and not just uniform)\n",
    "    the following line is very important. in autoregressive settings like LLMS, you mask future tokens from being used in the\n",
    "    context of a previous token. Hence why the masked fill line is present. In cases like sentiment\n",
    "    analysis, you may not want information to be masked, and rather have every token talk to every other\n",
    "    token in the sequence, so you would just delete the directly following masked fill line. \n",
    "    The autoregressive method with masked fill is known as a decoder attention block, sentiment like \n",
    "    full context without traingular masking is known as an encoder attention block\n",
    "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "    wei = F.softmax(wei, dim = -1) #(B, T, T)\n",
    "    v = self.value(x)\n",
    "    out = wei @ v\n",
    "    return out  '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618817\n"
     ]
    }
   ],
   "source": [
    "model = BigramLM(vocab_size)\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4175777d0527fdae249d0099fd8d3830575c26e0ce60d87f1e38087eba8437b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
